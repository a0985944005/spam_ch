{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好，這隻是個測試\n"
     ]
    }
   ],
   "source": [
    "from hanziconv import HanziConv\n",
    "line='你好，这只是个测试'\n",
    "print(HanziConv.toTraditional(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "當當專業圖書網提供央視焦點訪談節目嚮全國觀眾大力推薦的好書話說中國全套共捲光盤\n",
      "話說中國叢書係民族精神史詩重點齣版工程之一該工程已被列為年中央宣傳部項重點工程之一這套書用餘則故事位曆史人物幅以上圖片條曆史文化百科知識串起瞭泱泱中華年的曆史尤其是彆具匠心地以故事為經以圖片為緯的編排形式讓普通大眾乃至青少年讀者在閱讀一個個故事觀賞一幅幅圖畫的過程中輕鬆愉快地認知瞭中華五韆年的燦爛文明\n",
      "更多專業圖書敬請登陸當當專業圖書網本郵件采用國際列車發送可能是國內最先進的郵件發送軟件獨傢技術您想獲得網上最新發布的供求信息情報嗎您想獲取成韆上萬個網站上的最新商機嗎您想獲得企業最新公布的産品信息嗎您想獲知競爭對手在網絡上的營銷情況嗎您想獲得行業的最新信息嗎\n",
      "或許您已經給網絡上的浩瀚信息所淹沒您花費瞭大量的時間與精力卻所獲甚少\n",
      "供求情報通捕捉最新商機尋找最新客戶一旦擁有成功在手詳情請參考網站免費索取試用\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#這個方法可以彈藥另外存成記事本且要編碼維UTF8\n",
    "from hanziconv import HanziConv\n",
    "with open('data/spam/'+'5', 'r',encoding = 'gbk') as f:\n",
    "            # 过滤掉非中文字符\n",
    "            for line in f.readlines():\n",
    "                pattern = re.compile('[^\\u4e00-\\u9fa5]')\n",
    "                line = pattern.sub(\"\", line)\n",
    "                data=line.strip()\n",
    "                print(HanziConv.toTraditional(line))\n",
    "                # 将邮件中出现的词保存到wordsList中\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!usr/bin/env python  \n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "\"\"\" \n",
    "@author:yzk13 \n",
    "@time: 2018/03/25 \n",
    "\"\"\"\n",
    "import jieba\n",
    "import os\n",
    "\n",
    "class Classifier:\n",
    "    \"\"\"\n",
    "    分类器\n",
    "    \"\"\"\n",
    "    def getStopWords(self):\n",
    "        \"\"\"\n",
    "        获得停词列表\n",
    "        :return stopList 停词列表\n",
    "        \"\"\"\n",
    "        stopList = []\n",
    "        with open('data/stopWords.txt', 'r',encoding='gbk') as f:\n",
    "            for line in f.readlines():\n",
    "                # 去除末尾\\n\n",
    "                stopList.append(line[:-1])\n",
    "        return stopList\n",
    "\n",
    "    def getFileList(self, filePath):\n",
    "        \"\"\"\n",
    "        获取文件名称列表\n",
    "        :param: filePath 文件夹路径\n",
    "        :return fileNameList 文件名称列表\n",
    "        \"\"\"\n",
    "        fileNameList = os.listdir(filePath)\n",
    "        return fileNameList\n",
    "\n",
    "    def getWordsList(self, content, wordsList, stopList):\n",
    "        \"\"\"\n",
    "        获取词表词典, 由于要对多文本进行统计，所以这里不返回，一直在后面添加\n",
    "        :param content: 文本内容\n",
    "        :param wordsList: 词表\n",
    "        :param stopList: 停词表\n",
    "        \"\"\"\n",
    "        resultList = list(jieba.cut(content))\n",
    "        for result in resultList:\n",
    "            # 停词\n",
    "            if result not in stopList and result.strip() != '' and result != None:\n",
    "                if result not in wordsList:\n",
    "                    wordsList.append(result)\n",
    "\n",
    "    def listToDict(self, wordsList, wordsDict):\n",
    "        \"\"\"\n",
    "        list转dict\n",
    "        :param wordsList: 词列表\n",
    "        :param wordsDict: 词字典\n",
    "        \"\"\"\n",
    "        for item in wordsList:\n",
    "            if item in wordsDict.keys():\n",
    "                wordsDict[item] += 1\n",
    "            else:\n",
    "                wordsDict[item] = 1\n",
    "\n",
    "    def getProbWord(self, testDict, normalDict, spamDict, numNormal, numSpam):\n",
    "        \"\"\"\n",
    "        计算对分类结果影响最大的15个词\n",
    "        :param testDict: 测试数据字典\n",
    "        :param normalDict: 正常邮件字典\n",
    "        :param spamDict: 垃圾邮件字典\n",
    "        :param numNormal: 正常邮件的数量\n",
    "        :param numSpam: 垃圾邮件的数量\n",
    "        :return wordProbList: 对分类结果影响最大的15个词\n",
    "        \"\"\"\n",
    "        wordProbList = {}\n",
    "        for word, num in testDict.items():\n",
    "            # 当词不在垃圾邮件词表中，在正常邮件词表中，计算概率\n",
    "            if word in spamDict.keys() and word in normalDict.keys():\n",
    "                # 求类先验概率\n",
    "                # 正常邮件\n",
    "                pw_n = normalDict[word] / numNormal\n",
    "                # 垃圾邮件\n",
    "                pw_s = spamDict[word] / numSpam\n",
    "                ps_w = pw_s / (pw_s + pw_n)\n",
    "                wordProbList[word] = ps_w\n",
    "            # 当词在垃圾邮件词表中，不在正常邮件词表中，计算概率\n",
    "            if word in spamDict.keys() and word not in normalDict.keys():\n",
    "                pw_s = spamDict[word] / numSpam\n",
    "                pw_n = 0.01\n",
    "                ps_w = pw_s / (pw_s + pw_n)\n",
    "                wordProbList[word] = ps_w\n",
    "            # 当词在垃圾邮件词表中，而且在正常邮件词表中，计算概率\n",
    "            if word not in spamDict.keys() and word in normalDict.keys():\n",
    "                pw_s = 0.01\n",
    "                pw_n = normalDict[word] / numNormal\n",
    "                ps_w = pw_s / (pw_s + pw_n)\n",
    "                wordProbList[word] = ps_w\n",
    "            # 当词不在垃圾邮件词表中，也不在正常邮件词表中，计算概率\n",
    "            if word not in spamDict.keys() and word not in normalDict.keys():\n",
    "                wordProbList[word] = 0.5  # 0.4\n",
    "        sorted(wordProbList.items(), key=lambda d: d[1], reverse=True)[0:15]\n",
    "        return wordProbList\n",
    "\n",
    "    def calBayes(self, wordList, spamDict, normalDict):\n",
    "        \"\"\"\n",
    "        计算贝叶斯概率\n",
    "        :param wordList: 词表\n",
    "        :param spamDict: 垃圾邮件词语字典\n",
    "        :param normalDict: 正常邮件词语字典\n",
    "        :return: 概率\n",
    "        \"\"\"\n",
    "        ps_w = 1\n",
    "        ps_n = 1\n",
    "\n",
    "        with open('wordsProb.txt', 'a', encoding='gbk') as f:\n",
    "            for word, prob in wordList.items():\n",
    "                f.write(word + \":\" + str(prob) + \"\\n\")\n",
    "                ps_w *= prob\n",
    "                ps_n *= 1 - prob\n",
    "            p = ps_w / (ps_w + ps_n)\n",
    "        return p\n",
    "\n",
    "    def calAccuracy(self, testResult):\n",
    "        \"\"\"\n",
    "        计算精度\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        rightCount = 0\n",
    "        errorCount = 0\n",
    "        for name, catagory in testResult.items():\n",
    "            if (int(name) < 1000 and catagory == 0) or (int(name) > 1000 and catagory == 1):\n",
    "                rightCount += 1\n",
    "            else:\n",
    "                errorCount += 1\n",
    "        return rightCount / (rightCount + errorCount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\zxc98\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.709 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率：0.9387755102040817\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "if __name__ == '__main__':\n",
    "    classifier = Classifier()\n",
    "\n",
    "    # 定义变量\n",
    "    # 词频字典\n",
    "    normalDict = {}\n",
    "    spamDict = {}\n",
    "    testDict = {}\n",
    "\n",
    "    # 保存每封邮件中出现的词\n",
    "    wordsList = []\n",
    "    wordsDict = {}\n",
    "\n",
    "    # 保存结果\n",
    "    testResult = {}\n",
    "\n",
    "    # 获取文件夹文件文件名列表\n",
    "    listNormal = classifier.getFileList(r'data\\normal')\n",
    "    listSpam = classifier.getFileList(r'data\\spam')\n",
    "    # 测试集中文件名低于1000的为正常邮件\n",
    "    listTest = classifier.getFileList(r'data\\test')\n",
    "\n",
    "    # 正常邮件与垃圾邮件的数量\n",
    "    numNormal = len(listNormal)\n",
    "    numSpam = len(listSpam)\n",
    "\n",
    "    # 获得停词表\n",
    "    stopList = classifier.getStopWords()\n",
    "\n",
    "    # 获取正常邮件中的词频\n",
    "    for fileName in listNormal:\n",
    "        # 清空词表\n",
    "        wordsList.clear()\n",
    "        with open('data/normal/'+fileName, 'r',encoding='gbk') as f:\n",
    "            # 过滤掉非中文字符\n",
    "            for line in f.readlines():\n",
    "                pattern = re.compile('[^\\u4e00-\\u9fa5]')\n",
    "                line = pattern.sub(\"\", line)\n",
    "                # 将邮件中出现的词保存到wordsList中\n",
    "                classifier.getWordsList(line, wordsList, stopList)\n",
    "            # 统计每个词在所有邮件中出现的次数\n",
    "            classifier.listToDict(wordsList, wordsDict)\n",
    "    normalDict = wordsDict.copy()\n",
    "\n",
    "    wordsDict.clear()\n",
    "    # 获取垃圾邮件中的词频\n",
    "    for fileName in listSpam:\n",
    "        # 清空词表\n",
    "        wordsList.clear()\n",
    "        with open('data/spam/'+fileName, 'r',encoding='gbk') as f:\n",
    "            # 过滤掉非中文字符\n",
    "            for line in f.readlines():\n",
    "                pattern = re.compile('[^\\u4e00-\\u9fa5]')\n",
    "                line = pattern.sub(\"\", line)\n",
    "                # 将邮件中出现的词保存到wordsList中\n",
    "                classifier.getWordsList(line, wordsList, stopList)\n",
    "            # 统计每个词在所有邮件中出现的次数\n",
    "            classifier.listToDict(wordsList, wordsDict)\n",
    "    spamDict = wordsDict.copy()\n",
    "\n",
    "    # 将结果写入文件wordsProb中,判断是否存在，存在则删除，因为后面执行追加写入\n",
    "    if os.path.exists('wordsProb.txt'):\n",
    "        os.remove('wordsProb.txt')\n",
    "\n",
    "    # 测试邮件\n",
    "    for fileName in listTest:\n",
    "        testDict.clear()\n",
    "        wordsDict.clear()\n",
    "        wordsList.clear()\n",
    "        with open('data/test/' + fileName, 'r' ,encoding='gbk') as f:\n",
    "            # 过滤掉非中文字符\n",
    "            for line in f.readlines():\n",
    "                pattern = re.compile('[^\\u4e00-\\u9fa5]')\n",
    "                line = pattern.sub(\"\", line)\n",
    "                # 将邮件中出现的词保存到wordsList中\n",
    "                classifier.getWordsList(line, wordsList, stopList)\n",
    "            # 统计每个词在所有邮件中出现的次数\n",
    "            classifier.listToDict(wordsList, wordsDict)\n",
    "            testDict = wordsDict.copy()\n",
    "\n",
    "        # 计算每个文件中对分类影响最大的15个词\n",
    "        wordProbList = classifier.getProbWord(testDict, normalDict, spamDict, numNormal, numSpam)\n",
    "\n",
    "        # 对每封邮件得到的15个影响率最大的词计算贝叶斯概率\n",
    "        p = classifier.calBayes(wordProbList, spamDict, normalDict)\n",
    "        if (p > 0.5):\n",
    "            testResult[fileName] = 1\n",
    "        else:\n",
    "            testResult[fileName] = 0\n",
    "    testAccuracy = classifier.calAccuracy(testResult)\n",
    "    # 测试邮件分类结果\n",
    "    with open('result.txt', 'w' ,encoding='gbk') as f:\n",
    "        for i, ic in testResult.items():\n",
    "            f.write(str(i) + \",\" + str(ic) + \"\\n\")\n",
    "\n",
    "    print('准确率：' + str(testAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "files = [\"1.txt\",\"2.txt\"]\n",
    "list_ = \"\"\n",
    "all_text = []\n",
    "for file in files:         \n",
    "    with codecs.open(file, \"r\", \"utf-8\") as f:\n",
    "        text=[]\n",
    "        for line in f:\n",
    "            content = line.strip().split('\\t')\n",
    "            # 对每一行的内容进行处理\n",
    "            text = text+content\n",
    "        text = \" \".join(text)\n",
    "    all_text.append(text)\n",
    "data = pd.DataFrame(all_text,columns=['text'])\n",
    "data\n",
    "#         list_.append(pd.DataFrame(all_text,columns=['text']))  # Series对象也可\n",
    "# raw_data = pd.concat(all_text)\n",
    "# print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
