{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入一些套件與基本宣告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "spam total： 7775\n",
      "ham total： 7063\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "import codecs\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn import naive_bayes as bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "#簡轉繁\n",
    "from hanziconv import HanziConv\n",
    "\n",
    "#ham與spam的每封郵件list\n",
    "ham_text = []\n",
    "spam_text = []\n",
    "#ham與spam的數據集\n",
    "ham_data = []\n",
    "spam_data = []\n",
    "#取得目前工作目錄\n",
    "SaveDirectory = os.getcwd() \n",
    "#獲得文檔列表\n",
    "listham = os.listdir(SaveDirectory+'\\\\data\\\\normal')\n",
    "listspam = os.listdir(SaveDirectory+'\\\\data\\\\spam')\n",
    "#數據集數量\n",
    "\n",
    "print('spam total：',len(listspam))\n",
    "print('ham total：',len(listham))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將ham文本資料轉成繁體並存成DATAFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>這事兒還有續集 因為我姐她們買房子後來裝修的時候沒錢她婆婆他們就贊助瞭萬塊 這下一吵她婆婆就...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>越發覺得這個姐夫不錯瞭 那萬還是趕緊還瞭吧 標題姐姐快要生孩子瞭很感慨 發信站水木社區站內 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>或許某些農村吧 在一個當性彆勞動力富裕程度的地方還是可以理解的 標題姐姐快要生孩子瞭很感慨 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>最近經常看到你說一些很武斷的話 所謂保胎確實是要躺在床上不能動的尤其是先兆流産有齣血的時候 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>他們分手是說性格不閤 沒多久然後我們交往 還算比較愉快 當初因為他經常和聯係我曾跟他吵過多次...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  這事兒還有續集 因為我姐她們買房子後來裝修的時候沒錢她婆婆他們就贊助瞭萬塊 這下一吵她婆婆就...     0\n",
       "1  越發覺得這個姐夫不錯瞭 那萬還是趕緊還瞭吧 標題姐姐快要生孩子瞭很感慨 發信站水木社區站內 ...     0\n",
       "2  或許某些農村吧 在一個當性彆勞動力富裕程度的地方還是可以理解的 標題姐姐快要生孩子瞭很感慨 ...     0\n",
       "3  最近經常看到你說一些很武斷的話 所謂保胎確實是要躺在床上不能動的尤其是先兆流産有齣血的時候 ...     0\n",
       "4  他們分手是說性格不閤 沒多久然後我們交往 還算比較愉快 當初因為他經常和聯係我曾跟他吵過多次...     0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for fileName in listham:\n",
    "        with open('data/normal/'+fileName, 'r',encoding='gbk') as f:\n",
    "            #文本每一列儲存\n",
    "            text=[]\n",
    "            # 過濾非中文字符，正規化\n",
    "            for line in f.readlines():\n",
    "                pattern = re.compile('[^\\u4e00-\\u9fa5]') #正規化(去除非中文字符)\n",
    "                line = pattern.sub(\"\", line) #將其他字符取代為\"\"\n",
    "                content = line.strip().split() #去除一些空白.換行\n",
    "                text = text+content #儲存這個文本的內容\n",
    "            text = \" \".join(text) #將文本內容list加入全部文本的list\n",
    "            ham_text.append(HanziConv.toTraditional(text)) #簡轉繁\n",
    "ham_data = pd.DataFrame(ham_text,columns=['text']) #將list改成dataframe\n",
    "ham_data['label']='0' #將資料類別加上 0:ham 1:spam\n",
    "ham_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將spam文本資料轉成繁體並存成DATAFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>會員注冊邀請書 全國最低價域名空間大放送 國際頂級域名注冊驚暴價元年元年連續注冊年以上 國傢...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>你好 以茶會友以茶聯誼喝茶就喝安溪鐵觀音 本廠是專業生産批發各等級鐵觀音茶葉的購買聯係 在綫...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>尊敬的先生小姐 您好 首先感謝您抽時間查看以下信息 我公司一直緻力於研發生産銷售施工環氧樹脂...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>先生 你好 批發麻醉藥催情藥墮胎藥避孕藥飲料型噴霧型香煙型揮發型等各類國際國內製藥企業推齣的...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>貴公司負責人經理財務您好 我是深圳市創偉實業有限公司的我司實力雄厚有著良 好的社會關係也有部...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  會員注冊邀請書 全國最低價域名空間大放送 國際頂級域名注冊驚暴價元年元年連續注冊年以上 國傢...     1\n",
       "1  你好 以茶會友以茶聯誼喝茶就喝安溪鐵觀音 本廠是專業生産批發各等級鐵觀音茶葉的購買聯係 在綫...     1\n",
       "2  尊敬的先生小姐 您好 首先感謝您抽時間查看以下信息 我公司一直緻力於研發生産銷售施工環氧樹脂...     1\n",
       "3  先生 你好 批發麻醉藥催情藥墮胎藥避孕藥飲料型噴霧型香煙型揮發型等各類國際國內製藥企業推齣的...     1\n",
       "4  貴公司負責人經理財務您好 我是深圳市創偉實業有限公司的我司實力雄厚有著良 好的社會關係也有部...     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for fileName in listspam:\n",
    "        with open('data/spam/'+fileName, 'r',encoding='gbk') as f:\n",
    "            text=[]\n",
    "            # 过滤掉非中文字符\n",
    "            for line in f.readlines():\n",
    "                pattern = re.compile('[^\\u4e00-\\u9fa5]')\n",
    "                line = pattern.sub(\"\", line)\n",
    "                content = line.strip().split()\n",
    "                text = text+content\n",
    "            text = \" \".join(text)\n",
    "            spam_text.append(HanziConv.toTraditional(text))\n",
    "spam_data = pd.DataFrame(spam_text,columns=['text'])\n",
    "spam_data['label']='1'\n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將ham與spam組合並打亂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape： (22613, 2)\n",
      "spams in rows： 15550\n",
      "hams in rows： 7063\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>貴公司負責人經理財務您好 我公司是深圳市東訊實業有限公司我公司實力雄厚有著良好的社會關係 因...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>你好 深圳市源華實業有限公司現我公司可代開發票點數優惠 歡迎來電詢問謝謝 聯係人張偉明 聯係...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>若以下郵件打擾瞭您我在此深錶歉意請隨手刪除 貴公司負責人經理財務您好 深圳海利豐實業有限公司...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>她是我的本科同學我跟好的時候我就知道喜歡她我跟說你要是喜歡她就跟她在一起吧彆委屈自己的感情我...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>尊敬的公司財務負責人 您好深圳市聲威實業有限公司我公司在全國大中城市設有分公司因進項 較多現...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  貴公司負責人經理財務您好 我公司是深圳市東訊實業有限公司我公司實力雄厚有著良好的社會關係 因...     1\n",
       "1  你好 深圳市源華實業有限公司現我公司可代開發票點數優惠 歡迎來電詢問謝謝 聯係人張偉明 聯係...     1\n",
       "2  若以下郵件打擾瞭您我在此深錶歉意請隨手刪除 貴公司負責人經理財務您好 深圳海利豐實業有限公司...     1\n",
       "3  她是我的本科同學我跟好的時候我就知道喜歡她我跟說你要是喜歡她就跟她在一起吧彆委屈自己的感情我...     0\n",
       "4  尊敬的公司財務負責人 您好深圳市聲威實業有限公司我公司在全國大中城市設有分公司因進項 較多現...     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.concat([spam_data,ham_data],axis=0, ignore_index=True)  #將ham lsit與spam list串起來\n",
    "all_data = all_data.sample(frac=1).reset_index(drop=True) #sample frac = 1 是把數據都打亂 resr_index是把index也重設\n",
    "print('data shape：',all_data.shape)\n",
    "print('spams in rows：',all_data.loc[all_data['label']==\"1\"].shape[0])\n",
    "print('hams in rows：',all_data.loc[all_data['label']==\"0\"].shape[0])\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#載入 停用詞 (stopwords) 自動過濾掉某些字或詞\n",
    "stopwords = codecs.open(os.path.join(SaveDirectory+'\\data', 'stopwords_tr.txt'), 'r', 'utf-8').read().split('\\r\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將文本利用Jieba斷詞、過濾停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zxc98\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.588 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>貴 公司 負責人 經理 財務 您好 公司 東訊 實業 有限公司 公司 實力 雄厚 有著 良好...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>你好 源華 實業 有限公司 現 公司 可代開 發票 點數 優惠 歡迎 電詢 問謝謝 聯 係 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>以下 郵件 打擾 瞭 深 錶 歉意 請 隨手 刪除 貴 公司 負責人 經理 財務 您好 海利...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>本科 同學 好 知道 喜歡 說 喜歡 一起 彆 委屈 感情 說 隻 想 一起 轉眼 一年 半...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>尊敬 公司 財務 負責人 您好 聲威 實業 有限公司 公司 全國 大中城市 設有 分公司 因...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  貴 公司 負責人 經理 財務 您好 公司 東訊 實業 有限公司 公司 實力 雄厚 有著 良好...     1\n",
       "1  你好 源華 實業 有限公司 現 公司 可代開 發票 點數 優惠 歡迎 電詢 問謝謝 聯 係 ...     1\n",
       "2  以下 郵件 打擾 瞭 深 錶 歉意 請 隨手 刪除 貴 公司 負責人 經理 財務 您好 海利...     1\n",
       "3  本科 同學 好 知道 喜歡 說 喜歡 一起 彆 委屈 感情 說 隻 想 一起 轉眼 一年 半...     0\n",
       "4  尊敬 公司 財務 負責人 您好 聲威 實業 有限公司 公司 全國 大中城市 設有 分公司 因...     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_texts = []\n",
    "for text in all_data[\"text\"]:\n",
    "    words = []\n",
    "    seg_list = jieba.cut(text) #利用Jieba斷詞\n",
    "    for seg in seg_list: \n",
    "        # isalpha()檢測詞是否由字符組成，且不在停用詞的list中\n",
    "        if (seg.isalpha()) & (seg not in stopwords):\n",
    "            words.append(seg)\n",
    "    sentence = \" \".join(words)\n",
    "    processed_texts.append(sentence)\n",
    "all_data[\"text\"] = processed_texts #利用過濾且斷好的詞取代文本\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 計算HAM和SPAM 的 TF-IDF差異 DIFF\n",
    "## 值越大的代表他在SPAM的可能性更大\n",
    "「size_table」: 要選多少個重要的「詞」出來，等於決定特徵向量的維度數。Default:我設成200。 「ignore」: 字少於幾個以下就不要算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_key_list(all_data, size_table=200,ignore=2):\n",
    "    dict_spam_raw = dict()\n",
    "    dict_genuine_raw = dict()\n",
    "    dict_IDF = dict()\n",
    "\n",
    "    # 去除字母外的所有內容.\n",
    "    for i in range(all_data.shape[0]):\n",
    "        finds = all_data.iloc[i].text.split(\" \")\n",
    "        if all_data.iloc[i].label == '1':\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                try:\n",
    "                    dict_spam_raw[find] = dict_spam_raw[find] + 1\n",
    "                except:\t\n",
    "                    dict_spam_raw[find] = dict_spam_raw.get(find,1) #若是新的文字因為後面get找不到這個索引會返回逗點後面的值\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw.get(find,0)\n",
    "        else:\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                try:\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw[find] + 1\n",
    "                except:\t\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw.get(find,1)\n",
    "                    dict_spam_raw[find] = dict_spam_raw.get(find,0)\n",
    "\n",
    "        word_set = set()\n",
    "        for find in finds:\n",
    "            if not(find in word_set):\n",
    "                if len(find)<ignore: continue\n",
    "                try:\n",
    "                    dict_IDF[find] = dict_IDF[find] + 1\n",
    "                except:\t\n",
    "                    dict_IDF[find] = dict_IDF.get(find,1)\n",
    "            word_set.add(find)\n",
    "    word_df = pd.DataFrame(list(zip(dict_genuine_raw.keys(),dict_genuine_raw.values(),dict_spam_raw.values(),dict_IDF.values())))\n",
    "    word_df.columns = ['keyword','genuine','spam','IDF']\n",
    "    word_df['genuine'] = word_df['genuine'].astype('float')/all_data[all_data['label']=='0'].shape[0]\n",
    "    word_df['spam'] = word_df['spam'].astype('float')/all_data[all_data['label']=='1'].shape[0]\n",
    "    word_df['IDF'] = np.log10(word_df.shape[0]/word_df['IDF'].astype('float'))\n",
    "    word_df['genuine_IDF'] = word_df['genuine']*word_df['IDF']\n",
    "    word_df['spam_IDF'] = word_df['spam']*word_df['IDF']\n",
    "    word_df['diff']=word_df['spam_IDF']-word_df['genuine_IDF']\n",
    "    selected_spam_key = word_df.sort_values('diff',ascending=False)  \n",
    "    keyword_dict = dict()\n",
    "    i = 0\n",
    "    for word in selected_spam_key.head(size_table).keyword:\n",
    "        keyword_dict.update({word.strip():i})\n",
    "        i+=1\n",
    "    return keyword_dict   \n",
    "# build a tabu list based on the training data\n",
    "size_table = 200               # 多少特徵維度去分類SPAM\n",
    "word_len_ignored = 2            # 忽略那些比這個還要小的字詞\n",
    "keyword_dict=generate_key_list(all_data, size_table, word_len_ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "公司:0\n",
      "發票:1\n",
      "有限公司:2\n",
      "實業:3\n",
      "我司:4\n",
      "每月:5\n",
      "左右:6\n",
      "貴司:7\n",
      "銷售:8\n",
      "代理:9\n",
      "您好:10\n",
      "信息:11\n",
      "額度:12\n",
      "廣告:13\n",
      "增值:14\n",
      "優惠:15\n",
      "經理:16\n",
      "財務:17\n",
      "負責人:18\n",
      "運輸:19\n",
      "商祺:20\n",
      "網站:21\n",
      "順祝:22\n",
      "數量:23\n",
      "進項:24\n",
      "點數:25\n",
      "鄭重:26\n",
      "服務:27\n",
      "詳情:28\n",
      "分公司:29\n",
      "優惠代開:30\n",
      "大小:31\n",
      "普通:32\n",
      "電話:33\n",
      "承諾:34\n",
      "所用:35\n",
      "企業:36\n",
      "商品:37\n",
      "完成:38\n",
      "抵扣:39\n",
      "真票:40\n",
      "郵件:41\n",
      "郵箱:42\n",
      "有效:43\n",
      "雄厚:44\n",
      "一部分:45\n",
      "可上:46\n",
      "直接:47\n",
      "實力:48\n",
      "多現:49\n",
      "閤作:50\n",
      "社會關:51\n",
      "廣州:52\n",
      "地址:53\n",
      "洽商:54\n",
      "業務:55\n",
      "稅務局:56\n",
      "永久:57\n",
      "商討:58\n",
      "如須:59\n",
      "付款:60\n",
      "地稅:61\n",
      "稅電腦:62\n",
      "客戶:63\n",
      "疑慮:64\n",
      "普通商品:65\n",
      "一步:66\n",
      "歡迎:67\n",
      "國稅:68\n",
      "有著:69\n",
      "手機:70\n",
      "核對:71\n",
      "如貴司:72\n",
      "網查證:73\n",
      "現有:74\n",
      "管理:75\n",
      "免費:76\n",
      "良好:77\n",
      "低還:78\n",
      "東莞:79\n",
      "擔心:80\n",
      "數較:81\n",
      "作點:82\n",
      "注冊:83\n",
      "軟件:84\n",
      "提供:85\n",
      "銷售稅:86\n",
      "電小時:87\n",
      "外代:88\n",
      "海關:89\n",
      "網絡:90\n",
      "搜索:91\n",
      "代開:92\n",
      "繳款:93\n",
      "專用:94\n",
      "電腦:95\n",
      "機會:96\n",
      "查詢:97\n",
      "尊敬:98\n",
      "能夠:99\n",
      "全國:100\n",
      "建築:101\n",
      "先生:102\n",
      "資料:103\n",
      "售額度:104\n",
      "一次:105\n",
      "稅務:106\n",
      "金利:107\n",
      "嘉實業:108\n",
      "成立:109\n",
      "傳真:110\n",
      "支持:111\n",
      "使用:112\n",
      "功能:113\n",
      "中心:114\n",
      "票據:115\n",
      "多年:116\n",
      "地坪:117\n",
      "名稱:118\n",
      "各地:119\n",
      "電子:120\n",
      "部分:121\n",
      "發布:122\n",
      "形象:123\n",
      "最新:124\n",
      "群發:125\n",
      "所開:126\n",
      "網址:127\n",
      "你好:128\n",
      "設計:129\n",
      "電腦版:130\n",
      "代辦:131\n",
      "希望:132\n",
      "單據:133\n",
      "牢固:134\n",
      "建築安裝:135\n",
      "樹立:136\n",
      "進倉:137\n",
      "以誠信:138\n",
      "服務業:139\n",
      "設備:140\n",
      "州市:141\n",
      "服務廣告:142\n",
      "價格:143\n",
      "請電:144\n",
      "來電:145\n",
      "貿易:146\n",
      "核心思想:147\n",
      "統計:148\n",
      "稅率:149\n",
      "印刷:150\n",
      "堅持:151\n",
      "設有:152\n",
      "防爆:153\n",
      "優惠代:154\n",
      "如有:155\n",
      "郵址:156\n",
      "下載:157\n",
      "局驗證:158\n",
      "會員:159\n",
      "營銷:160\n",
      "華隆源:161\n",
      "租賃業:162\n",
      "需要:163\n",
      "地區:164\n",
      "各類:165\n",
      "稅額:166\n",
      "國內:167\n",
      "謝謝:168\n",
      "刪除:169\n",
      "打擾:170\n",
      "發送:171\n",
      "域名:172\n",
      "名錄:173\n",
      "精美:174\n",
      "稅發票:175\n",
      "即可:176\n",
      "點擊:177\n",
      "開還:178\n",
      "內河:179\n",
      "公路:180\n",
      "自動:181\n",
      "口業務:182\n",
      "驗證:183\n",
      "統一銷售:184\n",
      "源盛:185\n",
      "輸入:186\n",
      "服務行業:187\n",
      "神奇:188\n",
      "用戶:189\n",
      "用品:190\n",
      "稅點:191\n",
      "網查詢:192\n",
      "衡量:193\n",
      "建築安裝定:194\n",
      "因全國:195\n",
      "興隆:196\n",
      "額租賃:197\n",
      "人林錦龍:198\n",
      "輸服務:199\n"
     ]
    }
   ],
   "source": [
    "#排序越前面的代表出現在SPAM的機率越大\n",
    "for key,value in keyword_dict.items():\n",
    "    print('{key}:{value}'.format(key = key, value = value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將原本斷詞好的數據轉用TFIDF所過濾出來的詞轉換成稀疏矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Content(text, keyword_dict):\n",
    "    #判斷是否有此特徵\n",
    "    m = len(keyword_dict) #維度數量\n",
    "    res = np.int_(np.zeros(m)) #建置一個幾維度的向量\n",
    "    finds = text.split(\" \") #將文本內容切割(類似中文斷詞)\n",
    "    for find in finds:\n",
    "        try:\n",
    "            #若比對完有此特徵則特徵改為1\n",
    "            i = keyword_dict[find] \n",
    "            res[i]=1\n",
    "        except:\n",
    "            continue\n",
    "    return res\n",
    "def raw2feature(all_data,keyword_dict):\n",
    "    n_all_data = all_data.shape[0]\n",
    "    m = len(keyword_dict)\n",
    "    X_all_data = np.zeros((n_all_data,m));\n",
    "    Y_all_data = np.int_(all_data.label=='1')\n",
    "    for i in range(n_all_data):\n",
    "        X_all_data[i,:] = convert_Content(all_data.iloc[i].text, keyword_dict)\n",
    "    return [X_all_data,Y_all_data]\n",
    "     \n",
    "all_data_matrix=raw2feature(all_data,keyword_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用交叉驗證方式將數據切成訓練集與測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindate & testdata\n",
    "train_set, test_set, trainlabel, testlabel = train_test_split(all_data_matrix[0], all_data_matrix[1], test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:20351\n",
      "test:2262\n",
      "13976\n",
      "1574\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\"+str(len(train_set)))\n",
    "print(\"test:\"+str(len(test_set)))\n",
    "print(sum(trainlabel))\n",
    "print(sum(testlabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuarcy NBclassifier : 87.32％\n",
      "Training Accuarcy RF: 97.08％\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "def learn(train,trainlabel):\n",
    "    model_NB = GaussianNB(priors=None)\n",
    "    model_NB.fit(train_set, trainlabel)\n",
    "    Y_hat_NB = model_NB.predict(train_set)\n",
    "\n",
    "    model_RF = RandomForestClassifier(n_estimators=10, max_depth=None,\\\n",
    "                                 min_samples_split=2, random_state=0)\n",
    "    model_RF.fit(train_set, trainlabel)\n",
    "    Y_hat_RF = model_RF.predict(train_set)\n",
    "    \n",
    "    n=np.size(trainlabel)\n",
    "    print('Training Accuarcy NBclassifier : {:.2f}％'.format(sum(np.int_(Y_hat_NB==trainlabel))*100./n))\n",
    "    print('Training Accuarcy RF: {:.2f}％'.format(sum(np.int_(Y_hat_RF==trainlabel))*100./n))\n",
    "    return model_NB,model_RF\n",
    "# train the Random Forest and the Naive Bayes Model using training data\n",
    "model_NB,model_RF=learn(train_set,trainlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuarcy: 87.49％ (sklearn.naive_bayes)\n",
      "Testing Accuarcy: 96.29％ (sklearn.ensemble.forest)\n"
     ]
    }
   ],
   "source": [
    "def test_func(test,testlabel,model):\n",
    "    Y_hat = model.predict(test)\n",
    "    n=np.size(testlabel)\n",
    "    print ('Testing Accuarcy: {:.2f}％ ({})'.format(sum(np.int_(Y_hat==testlabel))*100./n,model.__module__))\n",
    "# Test Model using testing data\n",
    "test_func(test_set,testlabel,model_NB)\n",
    "test_func(test_set,testlabel,model_RF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
