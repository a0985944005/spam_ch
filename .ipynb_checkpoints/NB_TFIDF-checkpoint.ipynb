{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入一些套件與基本宣告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "spam total： 7775\n",
      "ham total： 7063\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import string\n",
    "import codecs\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn import naive_bayes as bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "#簡轉繁\n",
    "from hanziconv import HanziConv\n",
    "\n",
    "#ham與spam的每封郵件list\n",
    "ham_text = []\n",
    "spam_text = []\n",
    "#ham與spam的數據集\n",
    "ham_data = []\n",
    "spam_data = []\n",
    "#取得目前工作目錄\n",
    "SaveDirectory = os.getcwd() \n",
    "#獲得文檔列表\n",
    "listham = os.listdir(SaveDirectory+'\\\\data\\\\normal')\n",
    "listspam = os.listdir(SaveDirectory+'\\\\data\\\\spam')\n",
    "#數據集數量\n",
    "\n",
    "print('spam total：',len(listspam))\n",
    "print('ham total：',len(listham))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將ham文本資料轉成繁體並存成DATAFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileName in listham:\n",
    "        with open('data/normal/'+fileName, 'r',encoding='gbk') as f:\n",
    "            #文本每一列儲存\n",
    "            text=[]\n",
    "            # 過濾非中文字符，正規化\n",
    "            for line in f.readlines():\n",
    "                pattern = re.compile('[^\\u4e00-\\u9fa5]') #正規化(去除非中文字符)\n",
    "                line = pattern.sub(\"\", line) #將其他字符取代為\"\"\n",
    "                content = line.strip().split() #去除一些空白.換行\n",
    "                text = text+content #儲存這個文本的內容\n",
    "            text = \" \".join(text) #將文本內容list加入全部文本的list\n",
    "            ham_text.append(HanziConv.toTraditional(text)) #簡轉繁\n",
    "ham_data = pd.DataFrame(ham_text,columns=['text']) #將list改成dataframe\n",
    "ham_data['label']='0' #將資料類別加上 0:ham 1:spam\n",
    "ham_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將spam文本資料轉成繁體並存成DATAFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileName in listspam:\n",
    "        with open('data/spam/'+fileName, 'r',encoding='gbk') as f:\n",
    "            text=[]\n",
    "            # 过滤掉非中文字符\n",
    "            for line in f.readlines():\n",
    "                pattern = re.compile('[^\\u4e00-\\u9fa5]')\n",
    "                line = pattern.sub(\"\", line)\n",
    "                content = line.strip().split()\n",
    "                text = text+content\n",
    "            text = \" \".join(text)\n",
    "            spam_text.append(HanziConv.toTraditional(text))\n",
    "spam_data = pd.DataFrame(spam_text,columns=['text'])\n",
    "spam_data['label']='1'\n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將ham與spam組合並打亂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([spam_data,ham_data],axis=0, ignore_index=True)  #將ham lsit與spam list串起來\n",
    "all_data = all_data.sample(frac=1).reset_index(drop=True) #sample frac = 1 是把數據都打亂 resr_index是把index也重設\n",
    "print('data shape：',all_data.shape)\n",
    "print('spams in rows：',all_data.loc[all_data['label']==\"1\"].shape[0])\n",
    "print('hams in rows：',all_data.loc[all_data['label']==\"0\"].shape[0])\n",
    "all_data.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#載入 停用詞 (stopwords) 自動過濾掉某些字或詞\n",
    "stopwords = codecs.open(os.path.join(SaveDirectory+'\\data', 'stopwords_tr.txt'), 'r', 'utf-8').read().split('\\r\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將文本利用Jieba斷詞、過濾停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = []\n",
    "for text in all_data[\"text\"]:\n",
    "    words = []\n",
    "    seg_list = jieba.cut(text) #利用Jieba斷詞\n",
    "    for seg in seg_list: \n",
    "        # isalpha()檢測詞是否由字符組成，且不在停用詞的list中\n",
    "        if (seg.isalpha()) & (seg not in stopwords):\n",
    "            words.append(seg)\n",
    "    sentence = \" \".join(words)\n",
    "    processed_texts.append(sentence)\n",
    "all_data[\"text\"] = processed_texts #利用過濾且斷好的詞取代文本\n",
    "all_data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 計算HAM和SPAM 的 TF-IDF差異 DIFF\n",
    "## 值越大的代表他在SPAM的可能性更大\n",
    "「size_table」: 要選多少個重要的「詞」出來，等於決定特徵向量的維度數。Default:我設成200。 「ignore」: 字少於幾個以下就不要算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def generate_key_list(all_data, size_table=200,ignore=2):\n",
    "    dict_spam_raw = dict()\n",
    "    dict_genuine_raw = dict()\n",
    "    dict_IDF = dict()\n",
    "\n",
    "    # 去除字母外的所有內容.\n",
    "    for i in range(all_data.shape[0]):\n",
    "        finds = all_data.iloc[i].text.split(\" \")\n",
    "        if all_data.iloc[i].label == '1':\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                try:\n",
    "                    dict_spam_raw[find] = dict_spam_raw[find] + 1\n",
    "                except:\t\n",
    "                    dict_spam_raw[find] = dict_spam_raw.get(find,1) #若是新的文字因為後面get找不到這個索引會返回逗點後面的值\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw.get(find,0)\n",
    "        else:\n",
    "            for find in finds:\n",
    "                if len(find)<ignore: continue\n",
    "                try:\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw[find] + 1\n",
    "                except:\t\n",
    "                    dict_genuine_raw[find] = dict_genuine_raw.get(find,1)\n",
    "                    dict_spam_raw[find] = dict_spam_raw.get(find,0)\n",
    "\n",
    "        word_set = set()\n",
    "        for find in finds:\n",
    "            if not(find in word_set):\n",
    "                if len(find)<ignore: continue\n",
    "                try:\n",
    "                    dict_IDF[find] = dict_IDF[find] + 1\n",
    "                except:\t\n",
    "                    dict_IDF[find] = dict_IDF.get(find,1)\n",
    "            word_set.add(find)\n",
    "    word_df = pd.DataFrame(list(zip(dict_genuine_raw.keys(),dict_genuine_raw.values(),dict_spam_raw.values(),dict_IDF.values())))\n",
    "    word_df.columns = ['keyword','genuine','spam','IDF']\n",
    "    word_df['genuine'] = word_df['genuine'].astype('float')/all_data[all_data['label']=='0'].shape[0]\n",
    "    word_df['spam'] = word_df['spam'].astype('float')/all_data[all_data['label']=='1'].shape[0]\n",
    "    word_df['IDF'] = np.log10(word_df.shape[0]/word_df['IDF'].astype('float'))\n",
    "    word_df['genuine_IDF'] = word_df['genuine']*word_df['IDF']\n",
    "    word_df['spam_IDF'] = word_df['spam']*word_df['IDF']\n",
    "    word_df['diff']=word_df['spam_IDF']-word_df['genuine_IDF']\n",
    "    selected_spam_key = word_df.sort_values('diff',ascending=False)  \n",
    "    keyword_dict = dict()\n",
    "    i = 0\n",
    "    for word in selected_spam_key.head(size_table).keyword:\n",
    "        keyword_dict.update({word.strip():i})\n",
    "        i+=1\n",
    "    return keyword_dict   \n",
    "# build a tabu list based on the training data\n",
    "size_table = 200               # 多少特徵維度去分類SPAM\n",
    "word_len_ignored = 2            # 忽略那些比這個還要小的字詞\n",
    "keyword_dict=generate_key_list(all_data, size_table, word_len_ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#排序越前面的代表出現在SPAM的機率越大\n",
    "for key,value in keyword_dict.items():\n",
    "    print('{key}:{value}'.format(key = key, value = value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將原本斷詞好的數據轉用TFIDF所過濾出來的詞轉換成稀疏矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Content(text, keyword_dict):\n",
    "    #判斷是否有此特徵\n",
    "    m = len(keyword_dict) #維度數量\n",
    "    res = np.int_(np.zeros(m)) #建置一個幾維度的向量\n",
    "    finds = text.split(\" \") #將文本內容切割(類似中文斷詞)\n",
    "    for find in finds:\n",
    "        try:\n",
    "            #若比對完有此特徵則特徵改為1\n",
    "            i = keyword_dict[find] \n",
    "            res[i]=1\n",
    "        except:\n",
    "            continue\n",
    "    return res\n",
    "def raw2feature(all_data,keyword_dict):\n",
    "    n_all_data = all_data.shape[0]\n",
    "    m = len(keyword_dict)\n",
    "    X_all_data = np.zeros((n_all_data,m));\n",
    "    Y_all_data = np.int_(all_data.label=='1')\n",
    "    for i in range(n_all_data):\n",
    "        X_all_data[i,:] = convert_Content(all_data.iloc[i].text, keyword_dict)\n",
    "    return [X_all_data,Y_all_data]\n",
    "     \n",
    "all_data_matrix=raw2feature(all_data,keyword_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用交叉驗證方式將數據切成訓練集與測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindate & testdata\n",
    "train_set, test_set, trainlabel, testlabel = train_test_split(all_data_matrix[0], all_data_matrix[1], test_size = 0.15)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train:\"+str(len(train_set)))\n",
    "print(\"test:\"+str(len(test_set)))\n",
    "print(sum(trainlabel))\n",
    "print(sum(testlabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面是傳統監督式NB.RF的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "def learn(train,trainlabel):\n",
    "    model_NB = GaussianNB(priors=None)\n",
    "    model_NB.fit(train_set, trainlabel)\n",
    "    Y_hat_NB = model_NB.predict(train_set)\n",
    "\n",
    "    model_RF = RandomForestClassifier(n_estimators=200, max_depth=None,\\\n",
    "                                 min_samples_split=2, random_state=0)\n",
    "    model_RF.fit(train_set, trainlabel)\n",
    "    Y_hat_RF = model_RF.predict(train_set)\n",
    "    \n",
    "    n=np.size(trainlabel)\n",
    "    print('Training Accuarcy NBclassifier : {:.2f}％'.format(sum(np.int_(Y_hat_NB==trainlabel))*100./n))\n",
    "    print('Training Accuarcy RF: {:.2f}％'.format(sum(np.int_(Y_hat_RF==trainlabel))*100./n))\n",
    "    return model_NB,model_RF\n",
    "# train the Random Forest and the Naive Bayes Model using training data\n",
    "model_NB,model_RF=learn(train_set,trainlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_func(test,testlabel,model):\n",
    "    Y_hat = model.predict(test)\n",
    "    n=np.size(testlabel)\n",
    "    print ('Testing Accuarcy: {:.2f}％ ({})'.format(sum(np.int_(Y_hat==testlabel))*100./n,model.__module__))\n",
    "# Test Model using testing data\n",
    "test_func(test_set,testlabel,model_NB)\n",
    "test_func(test_set,testlabel,model_RF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [10386, 11128, 11870, 12612,13354]\n",
    "y = [0.8949, 0.8986, 0.8962, 0.9106,0.9329]\n",
    "y1 = [0.9391, 0.9496, 0.9626, 0.9668,0.9636]\n",
    "plt.xlim(10000, 14000)\n",
    "plt.ylim(0.8, 0.99)\n",
    "plt.plot(x, y, marker='.', mec='b', mfc='w' )\n",
    "plt.plot(x, y1, marker='.', mec='b', mfc='w' )\n",
    "plt.legend([\n",
    "    'NB classifier', \n",
    "    'RF classifier'\n",
    "])  # 让图例生效\n",
    "# plt.xticks(x, names, rotation=45)\n",
    "plt.margins(0)\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "plt.xlabel(u\"Train data\") #X轴标签\n",
    "plt.ylabel(\"Accuracy\") #Y轴标签\n",
    "plt.title(\"Performance of the two approaches\") #标题\n",
    "plt.savefig(\"classifier \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面是測試PU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #數據處理\n",
    "import numpy as np  #隨機取數\n",
    "import matplotlib.pyplot as plt   #繪圖\n",
    "#把繪圖套入ipython中的魔法函數%\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = 7,7   # 圖片大小\n",
    "plt.rcParams['font.size'] = 14         # 圖片文字大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入上面數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabel_ = trainlabel.tolist()\n",
    "trainlabel_ = pd.Series(trainlabel_)\n",
    "train_set_ = train_set.tolist()\n",
    "train_set_ = pd.DataFrame(train_set_)\n",
    "sum(trainlabel_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隱藏9成的數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保留原本的標籤，之後比對使用\n",
    "trainlabel_orig = trainlabel.copy()\n",
    "\n",
    "# 取消一些標記的數據\n",
    "hidden_size = 5936\n",
    "#loc可以選擇數據(把上面數量的POSTIVE DATA隱藏為UNLABELED DATA)\n",
    "trainlabel_.loc[\n",
    "    np.random.choice(\n",
    "        trainlabel_[trainlabel_ == 1].index, \n",
    "        replace = False, \n",
    "        size = hidden_size\n",
    "    )\n",
    "] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用基本的監督式隨機森林法來計算概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用一般的隨機森林算法\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators = 1000,  # 使用1000顆決策樹\n",
    "    n_jobs = -1           # 使用全部的CPU合運算\n",
    ")\n",
    "rf.fit(train_set, trainlabel_)\n",
    "\n",
    "# 儲存次方法給定的分數\n",
    "results = pd.DataFrame({\n",
    "    'truth'      : trainlabel_orig,   # 真實標籤\n",
    "    'label'      : trainlabel_,        # 預測標籤\n",
    "    'output_std' : rf.predict_proba(train_set)[:,1],   # 隨機森林分數(求屬於標籤1的概率)\n",
    "}, columns = ['truth', 'label', 'output_std'])\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PU-Bagging\n",
    "### 1.他是透過將正實例和未標記實例結合創建數據集，並進行替換\n",
    "### 2.將正數視為正實例，負數視為未標記實例\n",
    "### 3.將分類器應用於隨機樣本中未包含的人合未標記數據點，稱為OOB(out of bag)，並記錄其分數\n",
    "### 重複上面三個步驟，最後每個點OOB分數為每次分配分數的平均值\n",
    "\n",
    "#### 原本是決策樹但校果不好改隨機森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_set_\n",
    "y = trainlabel_\n",
    "# 使用1000棵決策樹\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "nestimators = 1000\n",
    "# estimator = DecisionTreeClassifier()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "estimator = RandomForestClassifier(\n",
    "    n_estimators = 1000,  # 使用1000顆決策樹\n",
    "    n_jobs = -1           # 使用全部的CPU合運算\n",
    ")\n",
    "# 追蹤正實例和未標記實例的索引\n",
    "iP = y[y > 0].index\n",
    "iU = y[y <= 0].index\n",
    "\n",
    "# 對每個數據點記錄他是OOB的次數\n",
    "num_oob = pd.DataFrame(np.zeros(shape = y.shape), index = y.index)\n",
    "\n",
    "# 記錄OOB分數的總合\n",
    "sum_oob = pd.DataFrame(np.zeros(shape = y.shape), index = y.index)\n",
    "\n",
    "for _ in range(nestimators):\n",
    "    # 獲取本輪未標記點的 bootstrap sample(引導樣本)\n",
    "    ib = np.random.choice(iU, replace=True, size = len(iP))\n",
    "\n",
    "    # 找到本輪的OOB數據\n",
    "    i_oob = list(set(iU) - set(ib))\n",
    "\n",
    "    # 獲取訓練數據 (所有正實例和 bootstrap sample(未標記實例)還有建立樹\n",
    "    Xb = X[y > 0].append(X.loc[ib])\n",
    "    yb = y[y > 0].append(y.loc[ib])\n",
    "    estimator.fit(Xb, yb)\n",
    "    \n",
    "    # 紀錄本輪OOB分數還有OOB次數\n",
    "    sum_oob.loc[i_oob, 0] += estimator.predict_proba(X.loc[i_oob])[:,1]\n",
    "    num_oob.loc[i_oob, 0] += 1\n",
    "\n",
    "# 最後儲存平均的OOB分數\n",
    "results['output_bag'] = sum_oob / num_oob\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[ib].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = range(3936, hidden_size, 100)\n",
    "y_std, y_bag, y_skb, y_stp, y_all = [], [], [], [], []\n",
    "for t in ts:\n",
    "    y_std.append(\n",
    "        results[results.label == 0].sort_values(\n",
    "            'output_std', ascending = False\n",
    "        ).head(t).truth.mean()\n",
    "    )\n",
    "    y_bag.append(\n",
    "        results[results.label == 0].sort_values(\n",
    "            'output_bag', ascending = False\n",
    "        ).head(t).truth.mean()\n",
    "    )\n",
    "#     y_skb.append(\n",
    "#         results[results.label == 0].sort_values(\n",
    "#             'output_skb', ascending = False\n",
    "#         ).head(t).truth.mean()\n",
    "#     )\n",
    "#     y_stp.append(\n",
    "#         results[results.label == 0].sort_values(\n",
    "#             'output_stp', ascending = False\n",
    "#         ).head(t).truth.mean()\n",
    "#     )\n",
    "#     y_all.append(\n",
    "#         results[results.label == 0].sort_values(\n",
    "#             'output_all', ascending = False\n",
    "#         ).head(t).truth.mean()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取RN跟P在訓練一個新分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveX = results[results.label == 0].sort_values(\n",
    "            'output_std', ascending = False\n",
    "        ).index\n",
    "trainlabel_.loc[positiveX[:5936]] = 1\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "purf = RandomForestClassifier(\n",
    "    n_estimators = 1000,  # 使用1000顆決策樹\n",
    "    n_jobs = -1           # 使用全部的CPU合運算\n",
    ")\n",
    "purf.fit(train_set, trainlabel_)\n",
    "trainlabel_.loc[positiveX[:5936]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveXpos = results[results.label == 1].index\n",
    "positiveXneg = results[results.label == 0].sort_values('output_std', ascending = True).index\n",
    "positiveXpos = trainlabel_.loc[positiveXpos[:]]\n",
    "positiveXneg = trainlabel_.loc[positiveXneg[:653]]\n",
    "positiveXlabel = positiveXpos+positiveXneg\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "purfposneg = RandomForestClassifier(\n",
    "    n_estimators = 1000,  # 使用1000顆決策樹\n",
    "    n_jobs = -1           # 使用全部的CPU合運算\n",
    ")\n",
    "purfposneg.fit(train_set[positiveXlabel.index], trainlabel_[positiveXlabel.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveXpu = results[results.label == 0].sort_values('output_bag', ascending = False).index\n",
    "trainlabel_.loc[positiveXpu[:5936]] = 1\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pubaggingrf = RandomForestClassifier(\n",
    "    n_estimators = 1000,  # 使用1000顆決策樹\n",
    "    n_jobs = -1           # 使用全部的CPU合運算\n",
    ")\n",
    "pubaggingrf.fit(train_set, trainlabel_)\n",
    "trainlabel_.loc[positiveXpu[:5936]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 效能圖\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['figure.figsize'] = 15, 8\n",
    "\n",
    "plt.plot(\n",
    "    ts, y_std,\n",
    "    ts, y_bag,\n",
    "    #ts, y_stp,\n",
    "    #ts, y_all,\n",
    "    lw = 5\n",
    ")\n",
    "\n",
    "vals = plt.gca().get_yticks()\n",
    "plt.yticks(vals, ['%.0f%%' % (v*100) for v in vals])\n",
    "plt.xlabel('Number of unlabeled data points chosen from the top rated')\n",
    "plt.ylabel('Percent of chosen that are secretly positive')\n",
    "plt.legend([\n",
    "    'Standard classifier',\n",
    "    'PU bagging',\n",
    "])\n",
    "ylim = plt.gca().get_ylim()\n",
    "plt.title('Performance of the three approaches and of their average')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面是實驗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_func(test,testlabel,model):\n",
    "    Y_hat = model.predict(test)\n",
    "    n=np.size(testlabel)\n",
    "    print ('Testing Accuarcy: {:.2f}％ ({})'.format(sum(np.int_(Y_hat==testlabel))*100./n,model.__module__))\n",
    "# Test Model using testing data\n",
    "test_func(test_set,testlabel,rf) #傳統監督式\n",
    "test_func(test_set,testlabel,purf) #PU提出後的改良版\n",
    "test_func(test_set,testlabel,pubaggingrf)\n",
    "test_func(test_set,testlabel,purfposneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = range(100, hidden_size, 200)\n",
    "y_std = []\n",
    "for t in ts:\n",
    "    y_std.append(\n",
    "        results[results.label == 0].sort_values(\n",
    "            'output_std', ascending = False\n",
    "        ).head(t).truth.mean()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下是將測試集導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = []\n",
    "testspam_text = []\n",
    "testham_text = []\n",
    "test_data = []\n",
    "listtest = os.listdir(SaveDirectory+'\\\\data\\\\test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileName in listtest:\n",
    "    if int(fileName)>1000:\n",
    "        with open('data/test/'+fileName, 'r',encoding='gbk') as f:\n",
    "            #文本每一列儲存\n",
    "            text=[]\n",
    "            # 過濾非中文字符，正規化\n",
    "            for line in f.readlines():\n",
    "                pattern = re.compile('[^\\u4e00-\\u9fa5]') #正規化(去除非中文字符)\n",
    "                line = pattern.sub(\"\", line) #將其他字符取代為\"\"\n",
    "                content = line.strip().split() #去除一些空白.換行\n",
    "                text = text+content #儲存這個文本的內容\n",
    "            text = \" \".join(text) #將文本內容list加入全部文本的list\n",
    "            testspam_text.append(HanziConv.toTraditional(text)) #簡轉繁\n",
    "    else:\n",
    "        with open('data/test/'+fileName, 'r',encoding='gbk') as f:\n",
    "            #文本每一列儲存\n",
    "            text=[]\n",
    "            # 過濾非中文字符，正規化\n",
    "            for line in f.readlines():\n",
    "                pattern = re.compile('[^\\u4e00-\\u9fa5]') #正規化(去除非中文字符)\n",
    "                line = pattern.sub(\"\", line) #將其他字符取代為\"\"\n",
    "                content = line.strip().split() #去除一些空白.換行\n",
    "                text = text+content #儲存這個文本的內容\n",
    "            text = \" \".join(text) #將文本內容list加入全部文本的list\n",
    "            testham_text.append(HanziConv.toTraditional(text)) #簡轉繁\n",
    "        \n",
    "            \n",
    "testspam_data = pd.DataFrame(testspam_text,columns=['text']) #將list改成dataframe\n",
    "testham_data = pd.DataFrame(testham_text,columns=['text']) #將list改成dataframe\n",
    "testspam_data['label']='1' #將資料類別加上 0:ham 1:spam\n",
    "testham_data['label']='0'\n",
    "test_data = pd.concat([testspam_data,testham_data])\n",
    "test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = []\n",
    "for text in test_data[\"text\"]:\n",
    "    words = []\n",
    "    seg_list = jieba.cut(text) #利用Jieba斷詞\n",
    "    for seg in seg_list: \n",
    "        # isalpha()檢測詞是否由字符組成，且不在停用詞的list中\n",
    "        if (seg.isalpha()) & (seg not in stopwords):\n",
    "            words.append(seg)\n",
    "    sentence = \" \".join(words)\n",
    "    processed_texts.append(sentence)\n",
    "test_data[\"text\"] = processed_texts #利用過濾且斷好的詞取代文本\n",
    "test_data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_matrix=raw2feature(test_data,keyword_dict)\n",
    "test_testset = test_data_matrix[0]\n",
    "test_testlabel = test_data_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(test,testlabel,model):\n",
    "    Y_hat = model.predict(test)\n",
    "    n=np.size(testlabel)\n",
    "    print ('Testing Accuarcy: {:.2f}％ ({})'.format(sum(np.int_(Y_hat==testlabel))*100./n,model.__module__))\n",
    "# Test Model using testing data\n",
    "test_func(test_testset,test_testlabel,rf)\n",
    "test_func(test_testset,test_testlabel,model_RF)\n",
    "test_func(test_testset,test_testlabel,pubaggingrf)\n",
    "test_func(test_testset,test_testlabel,purfposneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
